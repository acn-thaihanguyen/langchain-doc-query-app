{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import UnstructuredReader\n",
    "import nltk\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.readers.file import HTMLTagReader\n",
    "import os\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from llama_index.core import SimpleDirectoryReader, Settings\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from llama_index.core import download_loader, ServiceContext, VectorStoreIndex, StorageContext\n",
    "from pinecone import Pinecone\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Download the required NLTK resource\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "dir_reader = SimpleDirectoryReader(\n",
    "    input_dir=\"../data/\",\n",
    "    file_extractor={\".html\": UnstructuredReader()},\n",
    ")\n",
    "documents = dir_reader.load_data()\n",
    "print(f\"Number of documents: {len(documents)}\")\n",
    "\n",
    "\n",
    "node_parser = SimpleNodeParser.from_defaults(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "embed_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    embed_batch_size=100,\n",
    ")\n",
    "\n",
    "Settings.llm = OpenAI()\n",
    "Settings.embed_model = OpenAIEmbeddings()\n",
    "\n",
    "index_name = \"langchain-doc-query-app\"\n",
    "pc = Pinecone(\n",
    "    api_key=os.environ[\"PINECONE_API_KEY\"],\n",
    ")\n",
    "pinecone_index = pc.Index(name=index_name)\n",
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    storage_context=storage_context,\n",
    "    show_progress=True,\n",
    ")\n",
    "print(\"finished ingesting...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querry from created vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "**********\n",
      "**********\n",
      "Trace: query\n",
      "    |_query -> 3.500983 seconds\n",
      "      |_retrieve -> 1.801156 seconds\n",
      "        |_embedding -> 0.368373 seconds\n",
      "      |_synthesize -> 1.699074 seconds\n",
      "        |_templating -> 2.1e-05 seconds\n",
      "        |_llm -> 1.69062 seconds\n",
      "**********\n",
      "Agents are different models or systems that can be used for conversational purposes, such as ChatOpenAI, ChatAnthropic, ChatVertexAI, ChatCohere, ChatFireworks, ChatGroq, ChatMistralAI, and ChatOpenAI from TogetherAI.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone\n",
    "from llama_index.core.callbacks import LlamaDebugHandler, CallbackManager\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "pc = Pinecone(\n",
    "    api_key=os.environ[\"PINECONE_API_KEY\"],\n",
    ")\n",
    "\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager(handlers=[llama_debug])\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "Settings.embed_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "Settings.callback_manager = callback_manager\n",
    "\n",
    "pinecone_index = pc.Index(name=\"langchain-doc-query-app\")\n",
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "query = \"What is agents?\"\n",
    "res = index.as_query_engine().query(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_query -> 3.743066 seconds\n",
      "      |_retrieve -> 1.022354 seconds\n",
      "        |_embedding -> 0.306898 seconds\n",
      "      |_synthesize -> 2.720364 seconds\n",
      "        |_templating -> 1.1e-05 seconds\n",
      "        |_llm -> 2.714759 seconds\n",
      "**********\n",
      "To build a chatbot, you can start by creating a persistence layer around the model to store and retrieve conversation data. You can then enhance the chatbot's functionality by incorporating prompt templates. Prompt templates help structure user inputs for the chatbot to process effectively. Additionally, you can introduce system messages and customize responses based on user interactions. It's also beneficial to implement a message history feature to track and manage conversations with users effectively. By following these steps and gradually adding complexity to the chatbot's capabilities, you can create a more interactive and personalized conversational experience for users.\n"
     ]
    }
   ],
   "source": [
    "query = \"How to build a chat bot\"\n",
    "res = index.as_query_engine().query(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
